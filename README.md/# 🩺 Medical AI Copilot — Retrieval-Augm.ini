# ğŸ©º Medical AI Copilot â€” Retrieval-Augmented Generation (RAG)

## Overview
Medical AI Copilot is a Retrieval-Augmented Generation (RAG) system designed to answer medical questions using **trusted clinical documents** such as WHO, NICE, Ministry of Health guidelines, and OpenStax textbooks.

Instead of relying on general-purpose language model knowledge, the system retrieves relevant medical context from curated documents and generates **grounded, source-backed answers**.

> âš ï¸ This project is for **educational and research purposes only** and is **not a substitute for professional medical advice**.

---

## Problem Statement
Large Language Models can hallucinate medical information, which is risky in healthcare contexts.  
This project addresses that problem by:
- Restricting answers to **retrieved medical documents**
- Providing **explicit source attribution**
- Running fully **offline using a local LLM** for privacy and reproducibility

---

## System Architecture

Medical PDFs
â†“
Text Extraction & Chunking
â†“
Sentence Embeddings
â†“
FAISS Vector Index
â†“
Semantic Retrieval
â†“
Local LLM (Ollama)
â†“
Grounded Medical Answer + Sources


---

## Tech Stack
- **Language:** Python
- **Embeddings:** SentenceTransformers
- **Vector Store:** FAISS
- **LLM:** Ollama (local inference)
- **UI:** Streamlit
- **Domain:** Medical / Healthcare AI

---

## Key Features
- Retrieval-Augmented Generation (RAG)
- Source-grounded medical answers
- Local LLM inference (no API dependency)
- Unicode-safe medical text processing
- Interactive Streamlit UI
- Document-level source attribution

---

## Example Questions
- What is diabetes?
- What are the types of diabetes?
- What are the risk factors for type 2 diabetes?
- How is diabetes managed according to guidelines?

---

## Project Structure


medical-ai-copilot/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_docs/ # Original medical PDFs
â”‚ â”œâ”€â”€ processed_docs/ # Extracted & cleaned text
â”‚ â””â”€â”€ vector_store/ # FAISS index & metadata
â”‚
â”œâ”€â”€ embeddings/
â”‚ â”œâ”€â”€ chunk_text.py
â”‚ â”œâ”€â”€ embed_text.py
â”‚ â”œâ”€â”€ build_faiss_index.py
â”‚ â””â”€â”€ retrieve.py
â”‚
â”œâ”€â”€ backend/
â”‚ â””â”€â”€ rag_pipeline.py
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ app.py # Streamlit UI
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


---

## How It Works (RAG Flow)
1. Medical documents are chunked into smaller text segments
2. Each chunk is converted into dense embeddings
3. FAISS indexes the embeddings for fast similarity search
4. User queries retrieve top relevant chunks
5. A local LLM generates an answer using **only the retrieved context**
6. Sources are returned alongside the answer

---

## Limitations
- Local LLM inference is slower than cloud-based APIs
- Answers are limited to the scope of uploaded documents
- Not designed for real-time clinical decision-making

---

## Future Improvements
- FastAPI backend for deployment
- Switchable LLM support (local + API-based)
- Evaluation metrics for hallucination detection
- Support for additional medical domains

---

## Disclaimer
This system provides informational responses based on medical documents and **does not provide medical diagnosis or treatment advice**. Always consult a qualified healthcare professional.
